{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AAL Australia: 4Q Sales Report**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Data Wrangling**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1 Data Inspection**\n",
    "\n",
    "_It's the process of getting familiar with the data in order to identify quality and structure issues._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./Data/AAL_Q4-2020_Sales.csv\")\n",
    "\n",
    "print(\"First 5 rows of the DataFrame:\")\n",
    "print(df.head(5))\n",
    "\n",
    "print(f\"\\nLast 5 rows of the DataFrame:\")\n",
    "print(df.tail(5))\n",
    "\n",
    "print(\"\\nDataFrame Summary\")\n",
    "print(df.info())\n",
    "\n",
    "has_null = df.isnull().values.any()\n",
    "print(\"\\nDoes the DataFrame have any null value?:\", has_null)\n",
    "\n",
    "number_of_duplicates = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicates: {number_of_duplicates}\")\n",
    "\n",
    "print(\"\\nList of numerical columns:\")\n",
    "numerical_columns = df.select_dtypes(include=[\"number\"]).columns\n",
    "print(numerical_columns)\n",
    "\n",
    "print(\"\\nList of categorical columns:\")\n",
    "categorical_columns = df.select_dtypes(exclude=[\"number\"]).columns\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 Data Cleaning**\n",
    "\n",
    "_It's the process of handling missing data, removing duplicates, converting data types, trimming whitespace, correcting inconsistencies, standardizing formats, dealing with outliers, and validating accuracy._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling duplicates:\n",
    "if number_of_duplicates > 0:\n",
    "    df = df.drop_duplicates(keep=\"first\")\n",
    "\n",
    "# Handling missing data for numerical columns:\n",
    "for column in numerical_columns:\n",
    "    if df[column].isnull().any():\n",
    "        column_median = df[column].median()\n",
    "        df[column] = df[column].fillna(column_median)\n",
    "\n",
    "# Handling missing data for categorical columns:\n",
    "for column in categorical_columns:\n",
    "    if df[column].isnull().any():\n",
    "        df[column] = df[column].fillna(\"Unknown\")\n",
    "\n",
    "\n",
    "# Identify and correct misspelled words and unnecessary whitespace:\n",
    "for column in categorical_columns:\n",
    "    if column != \"Date\":\n",
    "        df[column] = df[column].str.strip()\n",
    "        unique_values = df[column].unique()\n",
    "        print(f\"{column} unique values: {unique_values}\")\n",
    "\n",
    "# Handling Outliers:\n",
    "SALES_COLUMN = \"Sales\"\n",
    "\n",
    "q1 = df[SALES_COLUMN].quantile(0.25)\n",
    "q3 = df[SALES_COLUMN].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# Tagging all the 'sales outliers' with True or False\n",
    "condition = (df[SALES_COLUMN] >= lower_bound) & (df[SALES_COLUMN] <= upper_bound)\n",
    "df[\"Sales_Outlier\"] = ~condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 Data Transformation**\n",
    "\n",
    "_It involves converting data from its original form into a format that is more suitable for analysis._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Transforming Date column to YYYY-MM-DD:\n",
    "DATE_COLUMN = \"Date\"\n",
    "df[DATE_COLUMN] = pd.to_datetime(df[DATE_COLUMN], format=\"%d-%b-%Y\")\n",
    "\n",
    "\n",
    "# Encoding categorical data into numerical:\n",
    "def encode(df: DataFrame, original_column: str, new_column: str):\n",
    "    df[new_column], _unique = pd.factorize(df[original_column])\n",
    "\n",
    "\n",
    "ORIGINAL_COLUMNS = [\"Time\", \"State\", \"Group\"]\n",
    "NEW_COLUMNS = [\"Numerical_Time\", \"Numerical_State\", \"Numerical_Group\"]\n",
    "\n",
    "for original_column, new_column in zip(ORIGINAL_COLUMNS, NEW_COLUMNS):\n",
    "    encode(df, original_column, new_column)\n",
    "\n",
    "\n",
    "# Binning Sales column to create Sales_Range:\n",
    "SALES_RANGE = \"Sales_Range\"\n",
    "\n",
    "min = df[SALES_COLUMN].min()\n",
    "q2 = df[SALES_COLUMN].quantile(0.5)\n",
    "max = df[SALES_COLUMN].max()\n",
    "\n",
    "bin_edges = [min, q1, q2, q3, max]\n",
    "bin_labels = [\"0-25%\", \"25%-50%\", \"50%-75%\", \"75%-100%\"]\n",
    "\n",
    "df[SALES_RANGE] = pd.cut(\n",
    "    df[SALES_COLUMN], bins=bin_edges, labels=bin_labels, include_lowest=True\n",
    ")\n",
    "\n",
    "# Normalization\n",
    "cleaned_df = df.copy(deep=True)\n",
    "normalized_df = cleaned_df.copy(deep=True)\n",
    "new_numerical_columns = normalized_df.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "SCALER = MinMaxScaler()\n",
    "\n",
    "normalized_df[new_numerical_columns] = SCALER.fit_transform(\n",
    "    normalized_df[new_numerical_columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Data Saving**\n",
    "\n",
    "_It involves creating a new DataFrame with the wrangled data and save it into a new csv file._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_df.to_csv(\"./Data/AAL_Q4-2020_Sales_Cleaned.csv\", index=False)\n",
    "# normalized_df.to_csv(\"./Data/AAL_Q4-2020_Sales_Cleaned_and_Normalized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Data Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Descriptive Statistical Analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_stats = cleaned_df[\"Sales\"].describe().round()\n",
    "print(cleaned_stats)\n",
    "\n",
    "\"\"\"\n",
    "The dataset of 7,560 sales records shows a broad range of sales amounts from $5,000 to $162,500, highlighting diverse\n",
    "transaction sizes.\n",
    "\n",
    "With an average sale of $45,014 but a high standard deviation of $32,254, the data reveals a wide spread of sales figures,\n",
    "indicating a mix of low, medium, and high transactions.\n",
    "\n",
    "The sales distribution is right-skewed, as shown by 25% of sales being $20,000 or below, the median at $35,000, and 75% of sales\n",
    "not exceeding $65,000, though some transactions reach up to $162,500, suggesting outlier or high-value sales.\n",
    "\n",
    "The insights point towards focusing strategies on enhancing the value of transactions in the lower quartiles, targeting\n",
    "medium-value transactions for growth, and learning from high-value sales to boost overall performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1.1 Box Plot Chart**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "boxprops = dict(facecolor=\"lightblue\")\n",
    "medianprops = dict(color=\"green\")\n",
    "meanlineprops = dict(color=\"red\", linestyle=\"--\")\n",
    "boxplot_elements = plt.boxplot(\n",
    "    cleaned_df[\"Sales\"],\n",
    "    vert=False,\n",
    "    patch_artist=True,\n",
    "    notch=True,\n",
    "    showmeans=True,\n",
    "    boxprops=boxprops,\n",
    "    meanline=True,\n",
    "    meanprops=meanlineprops,\n",
    "    medianprops=medianprops,\n",
    ")\n",
    "\n",
    "plt.title(\"Sales Box Plot\")\n",
    "plt.xlabel(\"Sales Amount\")\n",
    "plt.yticks([1], [\"Sales\"])\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "medians = boxplot_elements[\"medians\"][0].get_xydata()[1, 0]\n",
    "means = cleaned_df[\"Sales\"].mean()\n",
    "\n",
    "median_pos = medians + ((plt.xlim()[1] - plt.xlim()[0]) * 0.01)\n",
    "mean_pos = means + ((plt.xlim()[1] - plt.xlim()[0]) * 0.01)\n",
    "\n",
    "plt.text(\n",
    "    median_pos,\n",
    "    1.02,\n",
    "    f\"Median: {medians:.2f}\",\n",
    "    verticalalignment=\"center\",\n",
    "    color=\"green\",\n",
    ")\n",
    "plt.text(mean_pos, 0.98, f\"Mean: {means:.2f}\", verticalalignment=\"center\", color=\"red\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Ranking Analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df = cleaned_df.copy(deep=True)\n",
    "\n",
    "# Units:\n",
    "sales_by_unit = ranking_df.groupby(\"Unit\")[\"Sales\"].sum().reset_index()\n",
    "sorted_sales_by_unit = sales_by_unit.sort_values(\"Sales\", ascending=False)\n",
    "\n",
    "top_3_units = sorted_sales_by_unit.head(3)\n",
    "bottom_3_units = sorted_sales_by_unit.tail(3)\n",
    "\n",
    "print(\"\\nTop 3 STORES (Units) with the highest total sales:\")\n",
    "print(top_3_units)\n",
    "\n",
    "print(\"\\nBottom 3 STORES (Units) with the lowest total Sales:\")\n",
    "print(bottom_3_units.sort_values(\"Sales\", ascending=True))\n",
    "\n",
    "# State:\n",
    "sales_by_state = ranking_df.groupby(\"State\")[\"Sales\"].sum().reset_index()\n",
    "sorted_sales_by_state = sales_by_state.sort_values(\"Sales\", ascending=False)\n",
    "\n",
    "top_3_states = sorted_sales_by_state.head(3)\n",
    "bottom_3_states = sorted_sales_by_state.tail(3)\n",
    "\n",
    "print(\"\\nTop 3 STATES with the highest total sales:\")\n",
    "print(top_3_states)\n",
    "\n",
    "print(\"\\nBottom 3 STATES with the lowest total Sales:\")\n",
    "print(bottom_3_states.sort_values(\"Sales\", ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.1 Ranking Analysis Visualizations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# [Chart] Top 3 STORES (Units) with the highest total sales:\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(top_3_units[\"Unit\"], top_3_units[\"Sales\"], color=\"green\")\n",
    "\n",
    "plt.title(\"Top 3 Units with the Highest Total Sales\")\n",
    "plt.xlabel(\"Store Unit\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "\n",
    "plt.ylim([1_000_000, 15_000_000])\n",
    "plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f\"{int(x):,}\"))\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        yval,\n",
    "        f\"{int(yval):,}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()\n",
    "\n",
    "# [Chart] Top 3 STATES with the highest total sales:\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(top_3_states[\"State\"], top_3_states[\"Sales\"], color=\"Green\")\n",
    "\n",
    "plt.title(\"Top 3 States with the Highest Total Sales\")\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "\n",
    "plt.ylim([50_000_000, 110_000_000])\n",
    "plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f\"{int(x):,}\"))\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        yval,\n",
    "        f\"{int(yval):,}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 Pivot Tables**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = cleaned_df.copy(deep=True)\n",
    "\n",
    "pivot_df[\"Year\"] = pivot_df[\"Date\"].dt.year\n",
    "pivot_df[\"Month\"] = pivot_df[\"Date\"].dt.month\n",
    "\n",
    "pivot_table_1 = pd.pivot_table(\n",
    "    pivot_df,\n",
    "    values=\"Sales\",\n",
    "    index=[\"State\", \"Group\"],\n",
    "    columns=[\"Month\"],\n",
    "    aggfunc=\"sum\",\n",
    "    margins=True,\n",
    "    margins_name=\"Total\",\n",
    ")\n",
    "\n",
    "print(\"\\n1st Pivot table: Total sales by State, Group, and Month\")\n",
    "print(pivot_table_1)\n",
    "\n",
    "pivot_table_2 = pd.pivot_table(\n",
    "    pivot_df,\n",
    "    values=\"Sales\",\n",
    "    index=[\"State\", \"Group\"],\n",
    "    columns=[\"Month\"],\n",
    "    aggfunc=\"mean\",\n",
    "    margins=True,\n",
    "    margins_name=\"Average\",\n",
    ").round(2)\n",
    "\n",
    "print(\"\\n 2nd Pivot table: Average sales by State, Group, and Month\")\n",
    "print(pivot_table_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
